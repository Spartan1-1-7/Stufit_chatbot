{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033f79ab-09f0-43bf-9309-aaba9c0881c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (4.53.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (0.33.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn nltk sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a565d814-0b66-40c5-9a39-189786dc8c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Downloading langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.6-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.11.0-cp313-cp313-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.3-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.69-py3-none-any.whl (441 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.4.6-py3-none-any.whl (367 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading greenlet-3.2.3-cp313-cp313-win_amd64.whl (297 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.11.0-cp313-cp313-win_amd64.whl (129 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl (495 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, sniffio, PyPDF2, pydantic-core, orjson, jsonpointer, h11, greenlet, annotated-types, SQLAlchemy, requests-toolbelt, pydantic, jsonpatch, httpcore, anyio, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed PyPDF2-3.0.1 SQLAlchemy-2.0.41 annotated-types-0.7.0 anyio-4.9.0 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.26 langchain-core-0.3.69 langchain-text-splitters-0.3.8 langsmith-0.4.6 orjson-3.11.0 pydantic-2.11.7 pydantic-core-2.33.2 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "014d0412-0b47-4ef7-848d-66302bd4cfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf ---\n",
      "  Extracting text from 172 pages...\n",
      "Successfully extracted raw text from: ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf\n",
      "Initial cleaning complete for C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf. Cleaned length: 264579 characters.\n",
      "\n",
      "--- Processing: C:/Users/Ayush Jindal/Downloads/Wallach.pdf ---\n",
      "  Extracting text from 1884 pages...\n",
      "Successfully extracted raw text from: Wallach.pdf\n",
      "Initial cleaning complete for C:/Users/Ayush Jindal/Downloads/Wallach.pdf. Cleaned length: 3245859 characters.\n",
      "\n",
      "--- Processing: C:/Users/Ayush Jindal/Downloads/Blood Results in Clinical Practice_ A practical guide to interpreting blood test results - Graham Basten (2019, M&K Update Ltd).pdf ---\n",
      "  Extracting text from 119 pages...\n",
      "Successfully extracted raw text from: Blood Results in Clinical Practice_ A practical guide to interpreting blood test results - Graham Basten (2019, M&K Update Ltd).pdf\n",
      "Initial cleaning complete for C:/Users/Ayush Jindal/Downloads/Blood Results in Clinical Practice_ A practical guide to interpreting blood test results - Graham Basten (2019, M&K Update Ltd).pdf. Cleaned length: 188421 characters.\n",
      "\n",
      "--- Initial Text Cleaning Summary ---\n",
      "Book: C:/Users/Ayush Jindal/Downloads/ROU... | Length: 264579 chars\n",
      "Sample (first 1000 chars):\n",
      " Routine Blood Results Explained Routine Blood Results Explained By Andrew Blann Ph D FRCPath FIBMS By Andrew Blann PhD FRCPath FIBMS This edition first published 2022 st Edition 2006 2nd Edition 2007 3rd Edition 2013 4th Edition 2021 Cambridge Scholars Publishing Lady Stephenson Library, Newcastle upon Tyne, NE6 2PA, UK British Library Cataloguing in Publication Data A catalogue record for this book is available from the British Library Copyright © 2022 by Andrew Blann PhD FRCPath FIBMS All rights for this book reserved.\n",
      "\n",
      "No pa rt of this book may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying , recording or otherwise, without the prior permission of the copyright owner.\n",
      "\n",
      "ISBN (10): 1-5275-7702-3 ISBN (13): 978-1-5275-7702-2 CONTENTS Part 1: Haematology and Blood Transfusion Objectives and Scope .................................................................................\n",
      "\n",
      "5 Part 2: Immunology Objective\n",
      "--------------------------------------------------\n",
      "Book: C:/Users/Ayush Jindal/Downloads/Wal... | Length: 3245859 chars\n",
      "Sample (first 1000 chars):\n",
      " 11th Edition Wallach’s Interpretation of Diagnostic Tests11th Edition Wallach’s Interpretation of Diagnostic Tests L.\n",
      "\n",
      "V.\n",
      "\n",
      "Rao, PhD, FAACC Professor of Pathology University of Massachusetts Medical School Worcester, Massachusetts Executive Director, Science Quest Diagnostics, North Region Marlborough, Massachusetts L.\n",
      "\n",
      "Michael Snyder, MD ProfessorDepartment of Medicine and Pathology University of Massachusetts Medical School UMass Memorial Medical Center Worcester, Massachusetts Academic Associate Quest Diagnostics MA, LLC Marlborough, Massachusetts Executive Editor: Sharon Zinner Development Editor: Robin Richman Editorial Coordinator: Tim Rinehart Marketing Manager: Rachel Mante Leung Senior Production Project Manager: Alicia Jackson Design Coordinator: Terry Mallon Senior Manufacturing Coordinator: Beth Welsh Prepress Vendor: SPi Global Eleventh Edition Copyright © 2021 Wolters Kluwer Copyright © 2015 Wolters Kluwer.\n",
      "\n",
      "Copyright © 2011, 2007 Lippincott Williams & Wilkins, a Wolters K\n",
      "--------------------------------------------------\n",
      "Book: C:/Users/Ayush Jindal/Downloads/Blo... | Length: 188421 chars\n",
      "Sample (first 1000 chars):\n",
      " Blood Results in Clinical Practice A practical guide to interpreting blood test resultsFor the full range of M&K Publishing books please visit our website: ww.mkupdate.co.uk Blood Results in Clinical Practice A practical guide to interpreting blood test results Graham Basten 2nd edition Dr Graham Basten ISBN: 978-1-910451-16-8 First published 2013 Reprinted 2014, 2017 This revised and updated edition published 2019 All rights reserved.\n",
      "\n",
      "No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, without either the prior permission of the publishers or a licence permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency, 90 Tottenham Court Road, London, W1T 4LP.\n",
      "\n",
      "Permissions may be sought directly from M&K Publishing, phone: 01768 773030, fax: 01768 781099 or email: publishing@mkupdate.co.uk Any person who does any unauthorised act \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import pandas as pd # pandas is useful for organizing extracted data later\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the directory where your PDF books are located\n",
    "# Make sure your PDF files are in a 'books' subfolder within your ChatBotProject folder.\n",
    "pdf_dir = 'books'\n",
    "\n",
    "# List of PDF file names you want to process\n",
    "pdf_files = [\n",
    "    'C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf',\n",
    "    'C:/Users/Ayush Jindal/Downloads/Wallach.pdf',\n",
    "    'C:/Users/Ayush Jindal/Downloads/Blood Results in Clinical Practice_ A practical guide to interpreting blood test results - Graham Basten (2019, M&K Update Ltd).pdf'\n",
    "]\n",
    "\n",
    "# --- Text Extraction Function ---\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file page by page.\n",
    "    Includes error handling for missing files or extraction issues.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(reader.pages)\n",
    "            print(f\"  Extracting text from {num_pages} pages...\")\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text\n",
    "                except Exception as page_e:\n",
    "                    print(f\"    Warning: Could not extract text from page {page_num + 1} of {os.path.basename(pdf_path)}: {page_e}\")\n",
    "                    text += \"[PAGE_TEXT_EXTRACTION_ERROR]\"\n",
    "        print(f\"Successfully extracted raw text from: {os.path.basename(pdf_path)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'. Please check the path and filename.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening or reading PDF '{pdf_path}': {e}\")\n",
    "    return text\n",
    "\n",
    "# --- Initial Text Cleaning Function ---\n",
    "def clean_initial_pdf_text(text):\n",
    "    \"\"\"\n",
    "    Performs initial cleaning on raw extracted PDF text.\n",
    "    Removes common PDF artifacts and standardizes formatting.\n",
    "    \"\"\"\n",
    "    # 1. Remove source tags like or \n",
    "\n",
    "    # 2. Remove common PDF rendering artifacts like (cid:xx)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "\n",
    "    # 3. Remove specific repeating headers/footers based on observation of your PDFs\n",
    "    text = re.sub(r'Routine Blood\\n Results\\n Explained\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Routine Blood Results Explained', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Blood Results in\\n Clinical Practice', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Blood Results in Clinical Practice', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'A practical guide\\n to interpreting blood\\n test results', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4. Remove page numbers. This is heuristic and targets various page number patterns.\n",
    "    text = re.sub(r'^\\s*\\d+\\s*(\\n|$)', '\\n', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'(\\n|^)\\s*\\d+\\s*(\\n|$)', '\\n', text)\n",
    "    text = re.sub(r'Page \\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Chapter \\d+\\s*\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'^\\s*\\d{1,3}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*(viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\s*$', '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "    # 5. Remove TOC-like entries that remain, or prefaces/abbreviations\n",
    "    text = re.sub(r'^\\s*(Contents|Preface|Abbreviations|Introduction|Part \\d+|Chapter \\d+).*?\\b(?:vii|viii|ix|x|xi|xii|\\d+)\\s*$', '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "    # 6. Standardize whitespace: Replace multiple whitespaces (including newlines) with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'([.?!])\\s*(?=[A-Z0-9]|$)', r'\\1\\n\\n', text) # Add double newline after sentences\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text) # Reduce excessive newlines\n",
    "\n",
    "    # 7. Remove hyphenation from words split across lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # 8. Clean specific table artifacts from contents that might be extracted oddly\n",
    "    text = re.sub(r'\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"', r'\\1 \\2', text)\n",
    "    text = re.sub(r'Objectives and Scope\\n', '', text)\n",
    "    text = re.sub(r'Table \\d+\\.\\d+:.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'The following table:', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Mode LastWriteTime Length ---- ------------- ------', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "cleaned_texts_by_book = {}\n",
    "\n",
    "for pdf_file_name in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_dir, pdf_file_name)\n",
    "    print(f\"\\n--- Processing: {pdf_file_name} ---\")\n",
    "\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    if raw_text:\n",
    "        cleaned_text = clean_initial_pdf_text(raw_text)\n",
    "        cleaned_texts_by_book[pdf_file_name] = cleaned_text\n",
    "        print(f\"Initial cleaning complete for {pdf_file_name}. Cleaned length: {len(cleaned_text)} characters.\")\n",
    "    else:\n",
    "        print(f\"Skipping {pdf_file_name} due to extraction error or empty content.\")\n",
    "\n",
    "print(\"\\n--- Initial Text Cleaning Summary ---\")\n",
    "for book_name, text_content in cleaned_texts_by_book.items():\n",
    "    print(f\"Book: {book_name[:35]}... | Length: {len(text_content)} chars\")\n",
    "    print(\"Sample (first 1000 chars):\\n\", text_content[:1000])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68a06ba6-6e17-4440-af0f-dff16572a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ayush\n",
      "[nltk_data]     Jindal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ayush\n",
      "[nltk_data]     Jindal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ayush\n",
      "[nltk_data]     Jindal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f884c2-4ed9-4c73-bdb3-f55044b2b20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunking text for: C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf ---\n",
      "Generated 684 chunks for C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf.\n",
      "\n",
      "--- Chunking text for: C:/Users/Ayush Jindal/Downloads/Wallach.pdf ---\n",
      "Generated 8485 chunks for C:/Users/Ayush Jindal/Downloads/Wallach.pdf.\n",
      "\n",
      "--- Chunking text for: C:/Users/Ayush Jindal/Downloads/Blood Results in Clinical Practice_ A practical guide to interpreting blood test results - Graham Basten (2019, M&K Update Ltd).pdf ---\n",
      "Generated 485 chunks for C:/Users/Ayush Jindal/Downloads/Blood Results in Clinical Practice_ A practical guide to interpreting blood test results - Graham Basten (2019, M&K Update Ltd).pdf.\n",
      "\n",
      "--- Chunks DataFrame Head (from all books) ---\n",
      "                                           book_name  \\\n",
      "0  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "1  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "2  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "3  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "4  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "\n",
      "                                            chunk_id  \\\n",
      "0  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "1  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "2  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "3  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "4  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "\n",
      "                                          chunk_text  chunk_length  \n",
      "0  Routine Blood Results Explained Routine Blood ...           495  \n",
      "1  book is available from the British Library Cop...           128  \n",
      "2  No pa rt of this book may be reproduced, store...           440  \n",
      "3  5 Part 2: Immunology Objectives and Scope .......           487  \n",
      "4  92 : Calcium, Bone, and Mu sculo-Skeletal Dise...           485  \n",
      "\n",
      "Total number of chunks generated: 9654\n",
      "Average chunk length: 415.12 characters.\n",
      "\n",
      "--- Sample Chunks for Review ---\n",
      "\n",
      "Chunk ID: C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_RESULTS_EXPLAINED_-_ANDREW_BLANN_(2022,_CAMBRIDGE_SCHOLARS_PUB)_(1)_chunk_0000\n",
      "Book: C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf\n",
      "Text (495 chars):\n",
      "Routine Blood Results Explained Routine Blood Results Explained By Andrew Blann Ph D FRCPath FIBMS By Andrew Blann PhD FRCPath FIBMS This edition first published 2022 st Edition 2006 2nd Edition 2007 3rd Edition 2013 4th Edition 2021 Cambridge Scholars Publishing Lady Stephenson Library, Newcastle upon Tyne, NE6 2PA, UK British Library Cataloguing in Publication Data A catalogue record for this book is available from the British Library Copyright © 2022 by Andrew Blann PhD FRCPath FIBMS All\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Chunk ID: C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_RESULTS_EXPLAINED_-_ANDREW_BLANN_(2022,_CAMBRIDGE_SCHOLARS_PUB)_(1)_chunk_0001\n",
      "Book: C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf\n",
      "Text (128 chars):\n",
      "book is available from the British Library Copyright © 2022 by Andrew Blann PhD FRCPath FIBMS All rights for this book reserved.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Chunk ID: C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_RESULTS_EXPLAINED_-_ANDREW_BLANN_(2022,_CAMBRIDGE_SCHOLARS_PUB)_(1)_chunk_0002\n",
      "Book: C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf\n",
      "Text (440 chars):\n",
      "No pa rt of this book may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying , recording or otherwise, without the prior permission of the copyright owner.\n",
      "\n",
      "ISBN (10): 1-5275-7702-3 ISBN (13): 978-1-5275-7702-2 CONTENTS Part 1: Haematology and Blood Transfusion Objectives and Scope .................................................................................\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # Ensure word_tokenize is imported if needed for preprocess_text_for_chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Re-define or ensure cleaned_texts_by_book is available from Step 1 (it should be if previous cell ran)\n",
    "# This block is here ONLY as a fallback if the previous cell wasn't run successfully (which it now has)\n",
    "# So, in your live notebook, cleaned_texts_by_book will already be populated.\n",
    "if 'cleaned_texts_by_book' not in locals() or not cleaned_texts_by_book:\n",
    "    print(\"Error: 'cleaned_texts_by_book' dictionary not found or empty. Please run Step 1 successfully first.\")\n",
    "    # Fallback to dummy data for demonstration if previous step failed\n",
    "    # This dummy data has been created to mimic a slightly more realistic output for testing chunking\n",
    "    cleaned_texts_by_book = {\n",
    "        'ROUTINE BLOOD RESULTS EXPLAINED - ANDREW BLANN (2022, CAMBRIDGE SCHOLARS PUB) (1).pdf': \"\"\"INTRODUCTION: ROUTINE BLOOD RESULTS EXPLAINED. \"...it is estimated that the data received by clinicians from Medical Laboratories constitutes 70-80% of the information they rely on to make major medical decisions\" The Biomedical Scientist 2005:49; page 38. Blood tests are important as they provide three times as much information as do all other sources (history, examination, symptoms, imaging etc.) combined. Fortunately, the vast majority of routine blood tests, certainly in routine, emergency, and critical care medicine, fall easily into defined groups haematology (with blood transfusion), immunology, and biochemistry, together often described as Blood Science. The layout of the volume will therefore follow this pattern. Each of the major sections breaks down into individual chapters and concludes with a dedicated example. Knowledge is nothing without practice. Therefore, the book will conclude with case studies designed to help the practitioner. These cases will look at both primary and secondary care. What is done where? In some Pathology Departments, certain tests are done in the Haematology Laboratory, whilst in other Hospitals the same test may be performed in the Biochemistry or Immunology Laboratory, or perhaps a Department of Blood Science. Examples of this include iron studies, immunoglobulins, C-reactive protein (CRP), and testing for vitamin B12. These latter tests are done on serum obtained from whole blood that has not been anticoagulated, but some tests are performed on blood where clotting has been prevented by anticoagulants. The reader is referred to their own Pathology Service for the correct tube for the test and the destination of these requests. Overall, our colleagues in the Pathology Department, regardless of discipline, would far rather set the position clear in a phone call than go through the bother of phoning back that a fresh sample in the correct tube must be obtained. If in doubt - PHONE! A note on units. In the real world, of course, results are almost always described as the numbers themselves (e.g., a haemoglobin of 125 or a cholesterol of 5) instead of the more correct way its unit (i.e., 125 g/L and 8 mmol/L). This shorthand is generally accepted, and generally makes life considerable easier. It matters not so much that the correct unit of the average size of a red blood cell is described fully, for example, as 112 fL, or in shorthand simply as 112, but it does matter that the particular cell is much larger than can be expected in health, implying come pathological condition. The reference ranges. In defining ill-health, we generally use good health as a comparator. Thus, a healthy person can be expected to have a certain blood result profile. However, sometimes these values are not well established and are subject to variation. Furthermore, there are many normal (healthy) people whose blood result may not be in the expected range of values this does not necessarily mean they are ill. Therefore, the concept of being 'normal' may as well be 'desirable'. So, you could use a 'reference' range, or perhaps a 'target' range. However, for the purposes of this volume, the reference range will be cited. Haematology, biochemistry, and immunology are very quantitative sciences. Consequently, the reference range is important. The precise definition of the reference range in use at a particular hospital is crucial and may not be transferable to another hospital. This may be because of small differences in the technical manner in which tests are derived. Furthermore, reference ranges may well (or actually should) reflect the local population that the hospital serves, and this is important as different catchment populations may well vary considerably, especially in race and ethnicity.\n",
    "        \"\"\",\n",
    "        'Wallach.pdf': \"\"\"Wallach’s Interpretation of Diagnostic Tests. L. V. Rao, PhD, FAACC Professor of Pathology. University of Massachusetts Medical School. Worcester, Massachusetts. Executive Director, Science Quest Diagnostics, North Region. Marlborough, Massachusetts. L. Michael Snyder, MD Professor. Department of Medicine and Pathology. University of Massachusetts Medical School. UMass Memorial Medical Center. Worcester, Massachusetts. Academic Associate Quest Diagnostics MA, LLC. Marlborough, Massachusetts. Eleventh Edition Copyright © 2021 Wolters Kluwer. All rights reserved. The Kidney. Renal Function. Kidneys filter blood. Urea and creatinine are waste products. GFR measures kidney function. High creatinine means poor kidney function. Kidney failure is serious. Dialysis is a treatment. Electrolytes. Sodium and potassium are electrolytes. They control water balance. High potassium is dangerous. Low sodium can mean dehydration. Diabetes. Diabetes is high blood sugar. Insulin controls sugar. Type 1 and Type 2 diabetes exist. HbA1c measures long-term control. Hypoglycemia is low blood sugar. Complications include neuropathy. Foot problems are common. Cholesterol and heart disease. Lipids are fats. HDL is good cholesterol. LDL is bad cholesterol. Statins lower cholesterol. The body's organs and functions are incredibly complicated and interlinked. The book will therefore outline such links in terms of 'family groups'. For example, the kidney is responsible for supporting the production of red blood cells; when looking at the full blood count (FBC), it may therefore be helpful to look at the renal or kidney function. These connections are highlighted in the text by the LINK symbol. Blood tests seldom provide a diagnosis on their own; they are best used in conjunction with case history, X-rays, scans and other reports by allied health professionals. Given that reference ranges are specific to the machine which analyses the blood in a particular healthcare setting, no formal ranges are presented in this book. You must only use the range presented alongside the result or the range approved by your local healthcare setting. The values and interpretations used in the book are based on current national guidance. Readers should always seek definitive local approved guidance on diagnosis, treatment, additional blood tests to request and file, and repeat timeframes.\n",
    "        \"\"\",\n",
    "        'Blood Results in Clinical Practice_ A practical guide to interpreting blood test results - Graham Basten (2019, M&K Update Ltd).pdf': \"\"\"Blood Results in Clinical Practice. A practical guide to interpreting blood test results. Graham Basten. 2nd edition. Dr Graham Basten ISBN: 978-1-910451-16-8. First published 2013. This revised and updated edition published 2019. All rights reserved. The second edition of this book continues to use storytelling to aid understanding, and also introduces a new and unique 10-point system to help explain blood results. The use of storytelling has been significantly improved and refined following several years of feedback on the first edition. The purpose of the book has also evolved. Given that so many protocols and decisions can now be found online, and many NHS trusts and private providers produce similar flow charts, the character of the book has been adjusted: it is less formal than a biochemistry textbook while still containing more narrative than an online protocol. It provides an excellent, accessible introduction to blood tests and what they mean. It also enables advanced practitioners to reflect on and improve their practice, and includes new and updated sections of relevance to physiotherapists, paramedics, pharmacists and advanced nurse practitioners. The main audiences for the book are: 1) Undergraduate or postgraduate healthcare students 2) Healthcare professionals who need an essential handbook for quick reference or as an accessible text for a new area of practice (for example, new MSK First Contact or paramedic practitioners in primary and community care) 3) Patients, clients, friends and relatives who may wish to know more about what a particular blood test means. Throughout the book, storytelling is used. For instance, we will talk about 'fire engines' and 'police cars' to help explain what are essentially abstract concepts. When a storytelling device is being used, this will be flagged up in the text as 'storytelling'. The human body's organs and functions are incredibly complicated and interlinked. The book will therefore outline such links in terms of 'family groups'. For example, the kidney is responsible for supporting the production of red blood cells; when looking at the full blood count (FBC), it may therefore be helpful to look at the renal or kidney function. These connections are highlighted in the text by the LINK symbol. Blood tests seldom provide a diagnosis on their own; they are best used in conjunction with case history, X-rays, scans and other reports by allied health professionals. Given that reference ranges are specific to the machine which analyses the blood in a particular healthcare setting, no formal ranges are presented in this book. You must only use the range presented alongside the result or the range approved by your local healthcare setting. The values and interpretations used in the book are based on current national guidance. Readers should always seek definitive local approved guidance on diagnosis, treatment, additional blood tests to request and file, and repeat timeframes.\n",
    "        \"\"\"\n",
    "    }\n",
    "    # Apply basic cleaning to dummy text to match expected input from Step 1\n",
    "    for book_name in cleaned_texts_by_book:\n",
    "        cleaned_texts_by_book[book_name] = re.sub(r'\\s+', ' ', cleaned_texts_by_book[book_name]).strip()\n",
    "        cleaned_texts_by_book[book_name] = re.sub(r'\\n{2,}', '\\n\\n', cleaned_texts_by_book[book_name])\n",
    "\n",
    "\n",
    "# --- Advanced Text Preprocessing and Chunking Function ---\n",
    "def clean_and_chunk_text(book_text, book_title, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Applies further cleaning and splits text into smaller, overlapping chunks suitable for LLMs.\n",
    "    \"\"\"\n",
    "    # Apply light preprocessing\n",
    "    # Remove chapter/page number patterns from headers/footers that might remain\n",
    "    processed_text = re.sub(r'Chapter \\d+.*?(\\d+|vii|ix|x|xi)\\s*$', '', book_text, flags=re.MULTILINE)\n",
    "    processed_text = re.sub(r'(\\n|^)\\s*(Preface|Abbreviations|Introduction|Part \\d+|Contents)\\s*$', '', processed_text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip() # Standardize whitespace\n",
    "    processed_text = re.sub(r'\\n{3,}', '\\n\\n', processed_text) # Reduce excessive newlines\n",
    "    processed_text = re.sub(r'([.?!])\\s*(?=[A-Z0-9]|$)', r'\\1\\n\\n', processed_text) # Add double newline after sentences\n",
    "    processed_text = re.sub(r'\\n{3,}', '\\n\\n', processed_text) # Re-reduce excessive newlines after sentence splitting\n",
    "\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Prioritize splitting by larger units first\n",
    "    )\n",
    "\n",
    "    # Split the document into chunks\n",
    "    chunks = text_splitter.split_text(processed_text)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    chunks_with_metadata = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunks_with_metadata.append({\n",
    "            'book_name': book_title,\n",
    "            'chunk_id': f\"{os.path.splitext(book_title)[0].replace(' ', '_').replace('.', '').replace('__', '_')}_chunk_{i:04d}\", # Cleaned ID for potential vector DB use\n",
    "            'chunk_text': chunk,\n",
    "            'chunk_length': len(chunk)\n",
    "            # 'start_char': book_text.find(chunk) # Approximate start char - may not be accurate after extensive cleaning\n",
    "        })\n",
    "    return chunks_with_metadata\n",
    "\n",
    "# --- Main Chunking Process ---\n",
    "all_processed_chunks = []\n",
    "\n",
    "for book_name, text_content in cleaned_texts_by_book.items():\n",
    "    print(f\"\\n--- Chunking text for: {book_name} ---\")\n",
    "    chunks_for_book = clean_and_chunk_text(text_content, book_name, chunk_size=500, chunk_overlap=100) # Adjust chunk_size/overlap as needed\n",
    "    all_processed_chunks.extend(chunks_for_book)\n",
    "    print(f\"Generated {len(chunks_for_book)} chunks for {book_name}.\")\n",
    "\n",
    "# Create a DataFrame of all chunks with their metadata\n",
    "df_book_chunks = pd.DataFrame(all_processed_chunks)\n",
    "\n",
    "print(\"\\n--- Chunks DataFrame Head (from all books) ---\")\n",
    "print(df_book_chunks.head())\n",
    "print(f\"\\nTotal number of chunks generated: {len(df_book_chunks)}\")\n",
    "print(f\"Average chunk length: {df_book_chunks['chunk_length'].mean():.2f} characters.\")\n",
    "\n",
    "# Example: Display a few sample chunks for review\n",
    "if not df_book_chunks.empty:\n",
    "    print(\"\\n--- Sample Chunks for Review ---\")\n",
    "    for i in range(min(3, len(df_book_chunks))): # Display first 3 chunks\n",
    "        print(f\"\\nChunk ID: {df_book_chunks['chunk_id'].iloc[i]}\")\n",
    "        print(f\"Book: {df_book_chunks['book_name'].iloc[i]}\")\n",
    "        print(f\"Text ({df_book_chunks['chunk_length'].iloc[i]} chars):\")\n",
    "        print(df_book_chunks['chunk_text'].iloc[i])\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d46473-fd25-4751-a11f-7e3f1d181d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (4.53.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (0.33.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayush jindal\\desktop\\chatbotproject\\chatbot_env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "464656f3-51dc-440b-ab48-e9ec463b747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush Jindal\\Desktop\\ChatBotProject\\chatbot_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush Jindal\\Desktop\\ChatBotProject\\chatbot_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ayush Jindal\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "\n",
      "Generating embeddings for 9654 chunks. This may take some time for large datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█| 302/302 [05:49<00:00,  1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (9654, 384)\n",
      "\n",
      "--- Final Chunks DataFrame Head with Embeddings ---\n",
      "                                           book_name  \\\n",
      "0  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "1  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "2  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "3  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "4  C:/Users/Ayush Jindal/Downloads/ROUTINE BLOOD ...   \n",
      "\n",
      "                                            chunk_id  \\\n",
      "0  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "1  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "2  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "3  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "4  C:/Users/Ayush_Jindal/Downloads/ROUTINE_BLOOD_...   \n",
      "\n",
      "                                          chunk_text  chunk_length  \\\n",
      "0  Routine Blood Results Explained Routine Blood ...           495   \n",
      "1  book is available from the British Library Cop...           128   \n",
      "2  No pa rt of this book may be reproduced, store...           440   \n",
      "3  5 Part 2: Immunology Objectives and Scope .......           487   \n",
      "4  92 : Calcium, Bone, and Mu sculo-Skeletal Dise...           485   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.033440527, 0.016502352, 0.006819937, 0.039...  \n",
      "1  [-0.009590683, -0.026081773, -0.042963974, 0.0...  \n",
      "2  [0.007997773, 0.04616743, -0.07028714, -0.0291...  \n",
      "3  [-0.025172405, -0.029858258, -0.05281477, -0.0...  \n",
      "4  [-0.062301837, 0.054388817, -0.044433784, -0.0...  \n",
      "\n",
      "Final DataFrame shape with embeddings: (9654, 5)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd # Ensure pandas is imported if not already in the cell\n",
    "\n",
    "# Ensure df_book_chunks is available from Step 2 (Advanced Cleaning and Semantic Chunking)\n",
    "# If for some reason df_book_chunks is empty or not found, this block provides dummy data for demonstration.\n",
    "if 'df_book_chunks' not in locals() or df_book_chunks.empty:\n",
    "    print(\"Warning: 'df_book_chunks' DataFrame is empty or not found. Using dummy data for demonstration.\")\n",
    "    df_book_chunks = pd.DataFrame({\n",
    "        'book_name': ['Dummy Book 1.pdf', 'Dummy Book 1.pdf', 'Dummy Book 2.pdf'],\n",
    "        'chunk_id': ['dummy_book_1_chunk_0001', 'dummy_book_1_chunk_0002', 'dummy_book_2_chunk_0001'],\n",
    "        'chunk_text': [\n",
    "            'This chunk explains the full blood count (FBC) and its components like red blood cells and white blood cells.',\n",
    "            'Another chunk discusses coagulation, D-dimers, and anticoagulants like warfarin and heparin for thrombosis.',\n",
    "            'This section focuses on liver function tests, bilirubin, ALT, and AST, and their role in diagnosing jaundice.'\n",
    "        ],\n",
    "        'chunk_length': [120, 150, 130]\n",
    "    })\n",
    "    print(\"Dummy df_book_chunks created for demonstration.\")\n",
    "\n",
    "\n",
    "# Initialize a Sentence Transformer model\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "# 'all-MiniLM-L6-v2' is a good balance of size and performance for general-purpose embeddings.\n",
    "# This will download the model weights the first time you run this command, which may take a few minutes.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "print(f\"\\nGenerating embeddings for {len(df_book_chunks)} chunks. This may take some time for large datasets...\")\n",
    "# Convert the 'chunk_text' column to a list for the model's input\n",
    "chunk_texts_list = df_book_chunks['chunk_text'].tolist()\n",
    "\n",
    "# Generate embeddings. The model.encode method handles batching and GPU usage automatically if available.\n",
    "# Set convert_to_tensor=False to get NumPy arrays directly if you don't need PyTorch tensors.\n",
    "chunk_embeddings = model.encode(chunk_texts_list, show_progress_bar=True, convert_to_tensor=False)\n",
    "\n",
    "print(f\"Generated embeddings with shape: {chunk_embeddings.shape}\")\n",
    "\n",
    "# Add the embeddings as a new column to the DataFrame\n",
    "# Storing NumPy arrays directly in a DataFrame column is efficient.\n",
    "df_book_chunks['embedding'] = list(chunk_embeddings)\n",
    "\n",
    "print(\"\\n--- Final Chunks DataFrame Head with Embeddings ---\")\n",
    "print(df_book_chunks.head())\n",
    "print(f\"\\nFinal DataFrame shape with embeddings: {df_book_chunks.shape}\")\n",
    "\n",
    "# You can now optionally save this vectorized data. Parquet is an excellent format for DataFrames\n",
    "# with numerical array columns like embeddings, especially if you have many chunks.\n",
    "# df_book_chunks.to_parquet('vectorized_medical_book_chunks.parquet', index=False)\n",
    "# print(\"\\nVectorized book chunks saved to 'vectorized_medical_book_chunks.parquet'\")\n",
    "\n",
    "# This `df_book_chunks` DataFrame, with its 'book_name', 'chunk_id', 'chunk_text', and 'embedding' columns,\n",
    "# is the clean, vectorized data suitable for feeding into an LLM system. This data can be used for:\n",
    "# - Storing in a Vector Database for Retrieval Augmented Generation (RAG).\n",
    "# - Directly using for similarity search to find relevant information based on user queries.\n",
    "# - Fine-tuning an LLM (though this is more complex and typically requires much more data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06c80f0c-bddf-4b02-9e59-443a4bc5e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving to Parquet: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "Attempting to save as CSV as a fallback (embeddings will be stored as strings).\n",
      "Vectorized data saved as 'vectorized_medical_book_chunks.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "# Assuming df_book_chunks is your DataFrame containing the chunks and embeddings\n",
    "# from the previous \"Convert Chunks to Vector Form\" step.\n",
    "\n",
    "try:\n",
    "    df_book_chunks.to_parquet('vectorized_medical_book_chunks.parquet', index=False)\n",
    "    print(\"\\nVectorized data successfully saved as 'vectorized_medical_book_chunks.parquet'\")\n",
    "    print(f\"File saved to: {os.path.join(os.getcwd(), 'vectorized_medical_book_chunks.parquet')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to Parquet: {e}\")\n",
    "    print(\"Attempting to save as CSV as a fallback (embeddings will be stored as strings).\")\n",
    "    try:\n",
    "        df_book_chunks.to_csv('vectorized_medical_book_chunks.csv', index=False)\n",
    "        print(\"Vectorized data saved as 'vectorized_medical_book_chunks.csv'\")\n",
    "    except Exception as csv_e:\n",
    "        print(f\"Error saving to CSV: {csv_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f0159e-d087-40f5-8504-bbe50b2d2096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pyarrow-20.0.0-cp313-cp313-win_amd64.whl (25.7 MB)\n",
      "   ---------------------------------------- 0.0/25.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.7 MB 7.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.8/25.7 MB 4.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 4.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.4/25.7 MB 4.1 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.2/25.7 MB 4.0 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 5.0/25.7 MB 4.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.8/25.7 MB 4.0 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.6/25.7 MB 3.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 7.3/25.7 MB 3.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 7.6/25.7 MB 3.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.7/25.7 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.2/25.7 MB 3.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.7/25.7 MB 3.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.5/25.7 MB 3.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.0/25.7 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.8/25.7 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.6/25.7 MB 3.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 13.4/25.7 MB 3.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 14.2/25.7 MB 3.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 14.7/25.7 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.5/25.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 16.0/25.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 16.5/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 17.0/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.8/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.6/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.4/25.7 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.2/25.7 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.0/25.7 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.8/25.7 MB 3.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.5/25.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.1/25.7 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.9/25.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.7/25.7 MB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-20.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f6b51fe-2ce2-4030-bd61-16416bf1a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorized data successfully saved as 'vectorized_medical_book_chunks.parquet'\n",
      "File saved to: C:\\Users\\Ayush Jindal\\Desktop\\ChatBotProject\\chatbot_env\\vectorized_medical_book_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_book_chunks is your DataFrame containing the chunks and embeddings\n",
    "# from the previous \"Convert Chunks to Vector Form\" step.\n",
    "\n",
    "try:\n",
    "    df_book_chunks.to_parquet('vectorized_medical_book_chunks.parquet', index=False)\n",
    "    print(\"\\nVectorized data successfully saved as 'vectorized_medical_book_chunks.parquet'\")\n",
    "    print(f\"File saved to: {os.path.join(os.getcwd(), 'vectorized_medical_book_chunks.parquet')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to Parquet: {e}\")\n",
    "    print(\"Saving to CSV as a fallback failed or was not attempted due to prior success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdcf22-bf32-427f-b57e-066a20602455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ChatBot Project)",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
